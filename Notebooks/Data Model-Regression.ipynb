{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modeling using Regression Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "sns.set(style=\"darkgrid\")\n",
    "import cufflinks as cf\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.feature_selection import RFE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "# libraries for regularization\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_clean_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data into Training and Testing Set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df) :   \n",
    "    X = df.loc[: ,df.columns != 'int_rate']\n",
    "    Y = df['int_rate']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=10 )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Following are the columns selected as the best features from SFS feature selection since it gives us the least rmse value comparitively**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['loan_amnt', 'funded_amnt_inv','installment','loan_status','inq_last_6mths','pub_rec','revol_util','open_il_24m',\n",
    " 'open_rv_24m','all_util', 'inq_fi','acc_open_past_24mths', 'term', 'sub_grade', 'application_type','int_rate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(df_new) # splitting the new selected features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling data for modelling: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scaled_data(X_train, X_test): \n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = get_scaled_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining function to calculate scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model,predicted_val,true_val):\n",
    "    RSq = r2_score(true_val,predicted_val)\n",
    "    print('R Squared: ' + str(RSq))\n",
    "    MAE = mean_absolute_error(true_val,predicted_val)\n",
    "    print('MAE: ' + str(MAE))\n",
    "    RMS = np.sqrt(mean_squared_error(true_val,predicted_val))\n",
    "    print('RMS: ' + str(RMS))\n",
    "    MAPE = np.mean(np.abs((true_val - predicted_val) / true_val)) * 100\n",
    "    print('MAPE: ' + str(MAPE))\n",
    "    rmse = np.sqrt(mean_squared_error(predicted_val,true_val))\n",
    "    print('RMSE: ' + str(rmse))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Model: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(X_train, X_test, y_train, y_test):\n",
    "    global lm\n",
    "\n",
    "\n",
    "    lm = LinearRegression()\n",
    "    \n",
    "    lm.fit(X_train,y_train)\n",
    "    \n",
    "    reg_pred_train = lm.predict(X_train)\n",
    "    reg_pred_test = lm.predict(X_test)\n",
    "    print('Linear Regression Results:')\n",
    "    print('-Training Metrics-')\n",
    "    compute_metrics(lm,reg_pred_train,y_train)\n",
    "    print('-Testing Metrics-')\n",
    "    compute_metrics(lm,reg_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression Results:\n",
      "-Training Metrics-\n",
      "R Squared: 0.9582869051102821\n",
      "MAE: 0.7076410290941456\n",
      "RMS: 0.9866410485182598\n",
      "MAPE: 5.263060864961952\n",
      "RMSE: 0.9866410485182598\n",
      "\n",
      "\n",
      "-Testing Metrics-\n",
      "R Squared: 0.958331537898327\n",
      "MAE: 0.7082942393937717\n",
      "RMS: 0.987189365531264\n",
      "MAPE: 5.2731995329344965\n",
      "RMSE: 0.987189365531264\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_linear_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**According to our Base model we are getting a decent accuracy with a R-square of 0.95 for both traing and testing data using features selected by using Sequenrial Forward Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking alpha values for Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niles\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations. Duality gap: 241442.19062427664, tolerance: 3165.13719024015\n",
      "\n",
      "C:\\Users\\niles\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations. Duality gap: 354122.92919582396, tolerance: 3164.8035129708805\n",
      "\n",
      "C:\\Users\\niles\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:475: ConvergenceWarning:\n",
      "\n",
      "Objective did not converge. You might want to increase the number of iterations. Duality gap: 316219.56353540474, tolerance: 3165.5834662356597\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1e-08}\n",
      "-1.0394868620400075\n"
     ]
    }
   ],
   "source": [
    "lasso = Lasso()\n",
    "parameters = {'alpha':[1e-15,1e-10,1e-8,1e-4,1e-3,1e-2,1,5,10,20]}\n",
    "lasso_regressor = GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "lasso_regressor.fit(X_train, y_train)\n",
    "print(lasso_regressor.best_params_)\n",
    "print(lasso_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lasso_model(X_train, X_test, y_train, y_test):\n",
    "    global ls\n",
    "\n",
    "\n",
    "    ls = Lasso(alpha=1e-08,normalize=True)\n",
    "    \n",
    "    ls.fit(X_train,y_train)\n",
    "    \n",
    "    ls_pred_train = ls.predict(X_train)\n",
    "    ls_pred_test = ls.predict(X_test)\n",
    "    print('Linear Regression Results:')\n",
    "    print('-Training Metrics-')\n",
    "    compute_metrics(ls,ls_pred_train,y_train)\n",
    "    print('-Testing Metrics-')\n",
    "    compute_metrics(ls,ls_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lasso_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking alpha value for Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1}\n",
      "-1.039486862027001\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge()\n",
    "parameters = {'alpha':[1e-15,1e-10,1e-8,1e-4,1e-3,1e-2,1,5,10,20]}\n",
    "ridge_regressor = GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\n",
    "ridge_regressor.fit(X_train, y_train)\n",
    "print(ridge_regressor.best_params_)\n",
    "print(ridge_regressor.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ridge_model(X_train, X_test, y_train, y_test):\n",
    "    global ridgereg\n",
    "\n",
    "\n",
    "    ridgereg = Ridge(alpha=2,normalize=True,max_iter=1e5 )\n",
    "    \n",
    "    ridgereg.fit(X_train,y_train)\n",
    "    \n",
    "    rid_pred_train = ridgereg.predict(X_train)\n",
    "    rid_pred_test = ridgereg.predict(X_test)\n",
    "    print('Ridge Regression Results:')\n",
    "    print('-Training Metrics-')\n",
    "    compute_metrics(ridgereg,rid_pred_train,y_train)\n",
    "    print('-Testing Metrics-')\n",
    "    compute_metrics(ridgereg,rid_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Regression Results:\n",
      "-Training Metrics-\n",
      "R Squared: 0.5810784139454304\n",
      "MAE: 2.413464876753904\n",
      "RMS: 3.1267225291223406\n",
      "MAPE: 21.32695556672998\n",
      "RMSE: 3.1267225291223406\n",
      "\n",
      "\n",
      "-Testing Metrics-\n",
      "R Squared: 0.5812887170355248\n",
      "MAE: 2.416097748099795\n",
      "RMS: 3.129349460389965\n",
      "MAPE: 21.367408291249035\n",
      "RMSE: 3.129349460389965\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_ridge_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gbr_model(X_train, X_test, y_train, y_test):\n",
    "    global gbr\n",
    "\n",
    "    params = {'n_estimators': 50, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "    gbr = GradientBoostingRegressor(**params)\n",
    "    \n",
    "    gbr.fit(X_train,y_train)\n",
    "    \n",
    "    gbr_pred_train = gbr.predict(X_train)\n",
    "    gbr_pred_test = gbr.predict(X_test)\n",
    "    \n",
    "    print('Gradient Boosting Regressor Results:')\n",
    "    print('-Training Metrics-')\n",
    "    compute_metrics(gbr,gbr_pred_train,y_train)\n",
    "    print('-Testing Metrics-')\n",
    "    compute_metrics(gbr,gbr_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Results:\n",
      "-Training Metrics-\n",
      "R Squared: 0.606766623604821\n",
      "MAE: 2.3480857017714354\n",
      "RMS: 3.029340976966813\n",
      "MAPE: 21.04333938576813\n",
      "RMSE: 3.029340976966813\n",
      "\n",
      "\n",
      "-Testing Metrics-\n",
      "R Squared: 0.6068544950674457\n",
      "MAE: 2.3504982523685913\n",
      "RMS: 3.032308544000489\n",
      "MAPE: 21.083831434716917\n",
      "RMSE: 3.032308544000489\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_gbr_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_Search_GBR(X_train, X_test, y_train, y_test):\n",
    "            param_grid_rf = {'n_estimators': [50,70,90], 'max_depth': [5, 10, 50], 'min_samples_split': [2,6,8],\n",
    "          'learning_rate': [0.01,0.001], 'loss': ['ls']}\n",
    "            grid_rf = GridSearchCV(GradientBoostingRegressor(), param_grid=param_grid_rf, n_jobs=-1, cv=3, refit=True)\n",
    "            grid_rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_Search_GBR(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 50, 'max_depth': 4, 'min_samples_split': 2,\n",
    "          'learning_rate': 0.01, 'loss': 'ls'}\n",
    "gbr = GradientBoostingRegressor(**params)\n",
    "    \n",
    "gbr.fit(X_train,y_train)\n",
    "    \n",
    "gbr_pred_train = gbr.predict(X_train)\n",
    "gbr_pred_test = gbr.predict(X_test)\n",
    "    \n",
    "print('Gradient Boosting Regressor Results After Hyperparamter Tuning:')\n",
    "print('-Training Metrics-')\n",
    "compute_metrics(gbr,gbr_pred_train,y_train)\n",
    "print('-Testing Metrics-')\n",
    "compute_metrics(gbr,gbr_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rf_model(X_train, X_test, y_train, y_test):\n",
    "    global rf\n",
    "\n",
    "    rf = RandomForestRegressor()\n",
    "    \n",
    "    rf.fit(X_train,y_train)\n",
    "    \n",
    "    rf_pred_train = rf.predict(X_train)\n",
    "    rf_pred_test = rf.predict(X_test)\n",
    "    \n",
    "    print('Random Forest Regressor Results:')\n",
    "    print('-Training Metrics-')\n",
    "    compute_metrics(rf,rf_pred_train,y_train)\n",
    "    print('-Testing Metrics-')\n",
    "    compute_metrics(rf,rf_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\niles\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning:\n",
      "\n",
      "The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Results:\n",
      "-Training Metrics-\n",
      "R Squared: 0.9985273885319703\n",
      "MAE: 0.07206066870706884\n",
      "RMS: 0.18538178623194024\n",
      "MAPE: 0.4758223210544743\n",
      "RMSE: 0.18538178623194024\n",
      "\n",
      "\n",
      "-Testing Metrics-\n",
      "R Squared: 0.9919923118243674\n",
      "MAE: 0.18292295020181207\n",
      "RMS: 0.4327631855233944\n",
      "MAPE: 1.2107842576197914\n",
      "RMSE: 0.4327631855233944\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_rf_model(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parater tuning using Grid Search CV for Random Forest Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_Search_RF(X_train, X_test, y_train, y_test):\n",
    "            param_grid_rf = {'n_estimators': [5, 20, 70], 'max_depth': [5, 10, 50], 'oob_score': [True, False]}\n",
    "            grid_rf = GridSearchCV(RandomForestRegressor(), param_grid=param_grid_rf, n_jobs=-1, cv=3, refit=True)\n",
    "            grid_rf.fit(X_train, y_train)\n",
    "            print(grid_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search CV started...\n",
      "{'max_depth': 50, 'n_estimators': 70, 'oob_score': True}\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print('Grid Search CV started...')\n",
    "grid_Search_RF(X_train, X_test, y_train, y_test)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor Results After Hyperparamter Tuning:\n",
      "-Training Metrics-\n",
      "R Squared: 0.9989821486106442\n",
      "MAE: 0.06497871110881892\n",
      "RMS: 0.1541221860004354\n",
      "MAPE: 0.4288666918910649\n",
      "RMSE: 0.1541221860004354\n",
      "\n",
      "\n",
      "-Testing Metrics-\n",
      "R Squared: 0.992811034611263\n",
      "MAE: 0.17273787543187788\n",
      "RMS: 0.41004349704410886\n",
      "MAPE: 1.142819084368319\n",
      "RMSE: 0.41004349704410886\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "scaled_rf = RandomForestRegressor(n_estimators=70, max_depth=50, oob_score=True, n_jobs=-1)\n",
    "scaled_rf.fit(X_train, y_train)\n",
    "rf_pred_train = scaled_rf.predict(X_train)\n",
    "rf_pred_test = scaled_rf.predict(X_test)\n",
    "\n",
    "print('Random Forest Regressor Results After Hyperparamter Tuning:')\n",
    "print('-Training Metrics-')\n",
    "compute_metrics(scaled_rf,rf_pred_train,y_train)\n",
    "print('-Testing Metrics-')\n",
    "compute_metrics(scaled_rf,rf_pred_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model after model selection for predicting the interest rate is Random Forest Regressor**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
